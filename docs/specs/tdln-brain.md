alright dan‚Äîkicking off with crate 1/3: tdln-brain. this is the ‚Äúmind‚Äù layer that turns messy NL into a strictly-parsable, proof-friendly intent. i‚Äôm giving you a full, drop-in README + API map + examples + test plan so your IDE agent can scaffold it perfectly. üß†‚ú®

‚∏ª

tdln-brain ‚Äî Deterministic Cognitive Layer for LogLine OS

NL ‚Üí TDLN SemanticUnit ‚Üí canonical bytes (via json_atomic) ‚Üí happy Gate ‚Üí verifiable execution.

tdln-brain is the cognitive shim between LLMs and the LogLine kernel. It renders a typed cognitive context, asks a model for an intent, and guarantees you can parse it into a tdln_ast::SemanticUnit with zero ambiguity. Reasoning (free-form text) is separated from action (strict JSON). Same semantics ‚Üí same bytes ‚Üí same CID.

Why this exists

Typical ‚Äúagent‚Äù libraries push strings around and pray. We want:
	‚Ä¢	Strict output: JSON that parses into a SemanticUnit or it‚Äôs a hard error.
	‚Ä¢	Kernel awareness: constraints (policies) visible before generation, reducing Gate rejections.
	‚Ä¢	Deterministic canon: one source of truth for canonical bytes (delegates to json_atomic).
	‚Ä¢	Simple drivers: plug any model (cloud or local) via a tiny NeuralBackend trait.

TL;DR

use tdln_brain::{CognitiveContext, NeuralBackend, GenerationConfig, parser::parse_decision, Message, RawOutput, UsageMeta};
use tdln_ast::SemanticUnit;

// 1) Prepare context
let ctx = CognitiveContext {
    system_directive: "You‚Äôre LogLine‚Äôs TDLN brain. Output VALID JSON for a SemanticUnit.".into(),
    recall: vec!["User balance: 420".into()],
    history: vec![Message::user("grant to alice amount 100")],
    constraints: vec!["Never transfer > 500 without second approval".into()],
};

// 2) Render messages for the model
let messages = ctx.render();

// 3) Call any NeuralBackend you like (example: a mock)
struct Mock; impl NeuralBackend for Mock {
    fn model_id(&self) -> &str { "mock-tdln" }
    async fn generate(&self, _m: &[Message], _c: &GenerationConfig) -> Result<RawOutput, tdln_brain::BrainError> {
        Ok(RawOutput {
            content: r#"```json
{ "kind": "grant", "slots": { "to": "alice", "amount": 100 } }
```"#.into(),
            meta: UsageMeta { model_id: "mock-tdln".into(), ..Default::default() }
        })
    }
}

// 4) Generate & parse
let backend = Mock;
let raw = backend.generate(&messages, &GenerationConfig::default()).await?;
let decision = parse_decision(&raw.content, raw.meta)?;
let intent: SemanticUnit = decision.intent;

// 5) Done: `intent` is canonicalizable, provable, and Gate-friendly.


‚∏ª

Crate scope

In
	‚Ä¢	Typed cognitive context (CognitiveContext) ‚Üí rendered prompt (Vec<Message>)
	‚Ä¢	Model integration via NeuralBackend trait
	‚Ä¢	Strict parsing (parse_decision) with reasoning extraction
	‚Ä¢	Output shape: Decision { reasoning?, intent: SemanticUnit, meta }
	‚Ä¢	Helpful error model (BrainError)

Out (by design)
	‚Ä¢	Policy decisions (that‚Äôs tdln-gate)
	‚Ä¢	Transport/wire (that‚Äôs ubl-sirp)
	‚Ä¢	Ledger writes (that‚Äôs ubl-ledger)
	‚Ä¢	Full agent loop (that‚Äôs ubl-office)

‚∏ª

Features
	‚Ä¢	http-drivers (optional): includes a minimal reqwest-based driver you can adapt.
	‚Ä¢	std only (no no_std targets here).
	‚Ä¢	No unsafe.

[features]
default = []
http-drivers = ["dep:reqwest"]


‚∏ª

Public API (stable v0.1)

Data types

#[derive(Clone, Debug, Serialize, Deserialize)]
pub struct Message { pub role: String, pub content: String }
impl Message {
  pub fn system(s: impl Into<String>) -> Self; 
  pub fn user(s: impl Into<String>) -> Self; 
  pub fn assistant(s: impl Into<String>) -> Self;
}

#[derive(Default, Clone, Debug, Serialize, Deserialize)]
pub struct CognitiveContext {
  pub system_directive: String,
  pub recall: Vec<String>,
  pub history: Vec<Message>,
  pub constraints: Vec<String>,
}
impl CognitiveContext { pub fn render(&self) -> Vec<Message>; }

#[derive(Clone, Debug, Default)]
pub struct UsageMeta { pub input_tokens: u32, pub output_tokens: u32, pub model_id: String }

pub struct RawOutput { pub content: String, pub meta: UsageMeta }

#[derive(Debug)]
pub struct Decision {
  pub reasoning: Option<String>,
  pub intent: tdln_ast::SemanticUnit,
  pub meta: UsageMeta,
}

#[derive(Debug, thiserror::Error)]
pub enum BrainError {
  #[error("provider error: {0}")] Provider(String),
  #[error("hallucination: output was not valid TDLN: {0}")] Hallucination(String),
  #[error("context window exceeded")] ContextOverflow,
  #[error("parsing error: {0}")] Parsing(String),
}

Trait for model providers

#[async_trait::async_trait]
pub trait NeuralBackend: Send + Sync {
  fn model_id(&self) -> &str;
  async fn generate(&self, messages: &[Message], cfg: &GenerationConfig)
    -> Result<RawOutput, BrainError>;
}

#[derive(Clone, Debug)]
pub struct GenerationConfig {
  pub temperature: f32,
  pub max_tokens: Option<u32>,
  pub require_reasoning: bool,
}
impl Default for GenerationConfig { /* temp=0.0, max_tokens=Some(1024) */ }

The parser

pub mod parser {
  use super::*;
  /// Extracts a JSON block (supports ```json fences) and parses into SemanticUnit.
  pub fn parse_decision(raw: &str, meta: UsageMeta) -> Result<Decision, BrainError>;
}


‚∏ª

Prompting model (how CognitiveContext::render() works)

render() builds a single system message packing:
	‚Ä¢	Your directive (role + tone + boundaries)
	‚Ä¢	Constraints (kernel policies the model must respect)
	‚Ä¢	Relevant memory (recall)
	‚Ä¢	Then appends your recent history

This cuts Gate rejections because the model knows the rules before proposing an action.

Example of system scaffold rendered:

IDENTITY: agent/logline
You output VALID JSON for a TDLN SemanticUnit. No extra prose.

### SYSTEM PROTOCOL ###
- Output a single JSON object with fields: kind, slots
- Example: {"kind":"grant","slots":{"to":"alice","amount":100}}

### ACTIVE KERNEL CONSTRAINTS ###
- Never transfer > 500 without secondary approval
- Read-only on /invoices/*

### RELEVANT MEMORY (RECALL) ###
- User balance: 420


‚∏ª

Canon bytes & CID (interoperability invariant)

tdln-brain does not compute CIDs itself; it produces a SemanticUnit that downstream crates (compiler/proof/gate) can:
	‚Ä¢	Canonicalize via json_atomic::canonize
	‚Ä¢	Hash via ubl_crypto::blake3_cid
	‚Ä¢	Prove via tdln-proof

This keeps one source of truth for canonical bytes across the whole stack.

‚∏ª

Examples

1) Mock end-to-end (no network)

# async fn demo() -> anyhow::Result<()> {
use tdln_brain::*;
use tdln_ast::SemanticUnit;

let ctx = CognitiveContext {
  system_directive: "Emit a valid TDLN SemanticUnit JSON.".into(),
  recall: vec![],
  history: vec![Message::user("grant to alice amount 100")],
  constraints: vec!["Never exceed 500 without approval".into()],
};
let messages = ctx.render();

struct Mock; impl NeuralBackend for Mock {
  fn model_id(&self)->&str{"mock"}
  async fn generate(&self,_:&[Message],_:&GenerationConfig)->Result<RawOutput,BrainError>{
    Ok(RawOutput{content:r#"{"kind":"grant","slots":{"to":"alice","amount":100}}"#.into(),meta:UsageMeta::default()})
  }
}
let backend = Mock;
let raw = backend.generate(&messages,&GenerationConfig::default()).await?;
let decision = parser::parse_decision(&raw.content, raw.meta)?;
assert_eq!(decision.intent.kind(), "grant");
# Ok(()) }

2) OpenAI driver (optional, http-drivers)

Provide a simple providers::openai::OpenAiDriver you can wire like:

#[cfg(feature="http-drivers")]
use tdln_brain::providers::openai::OpenAiDriver;

#[cfg(feature="http-drivers")]
# async fn demo(api_key:String)->anyhow::Result<()> {
let driver = OpenAiDriver::new(api_key, "gpt-4o-mini".into());
let decision = {
  let ctx = CognitiveContext { system_directive: "...".into(), ..Default::default() };
  let msgs = ctx.render();
  let raw = driver.generate(&msgs, &GenerationConfig::default()).await?;
  parser::parse_decision(&raw.content, raw.meta)?
};
// decision.intent ‚Üí pass to gate/ledger
# Ok(()) }

If the model supports JSON mode, the driver requests it. Otherwise, the parser tolerates prose + fenced blocks and still extracts the JSON.

‚∏ª

Error model
	‚Ä¢	Provider(..): transport, API, timeouts, HTTP.
	‚Ä¢	Hallucination(..): we got text, but not a valid SemanticUnit. (You‚Äôll see a precise serde error + a short context.)
	‚Ä¢	Parsing(..): rare‚Äîmalformed JSON we couldn‚Äôt recover from.
	‚Ä¢	ContextOverflow: use this to signal your agent runtime to ‚Äúdream‚Äù / compress memory (ubl-office).

‚∏ª

Tests you should have in tests/
	‚Ä¢	parses_clean_json: raw {"kind":...} works.
	‚Ä¢	parses_fenced_json: markdown-fenced JSON works, reasoning extracted.
	‚Ä¢	rejects_invalid_shape: wrong field types ‚Üí Hallucination.
	‚Ä¢	constraints_appear_in_system_prompt: ensure render() includes constraints.
	‚Ä¢	stable_render: rendering same context yields identical messages (idempotence).
	‚Ä¢	(optional) fuzz parser with tiny corpus‚Äîalready covered at workspace level; not mandatory here.

‚∏ª

Security & invariants
	‚Ä¢	#![forbid(unsafe_code)]
	‚Ä¢	No implicit decisions: if we can‚Äôt parse strict JSON ‚Üí hard error.
	‚Ä¢	Never insert model output directly to IO/FS/network‚Äîtdln-brain only emits AST; the Gate decides.
	‚Ä¢	Canon chain remains downstream; don‚Äôt hash here.

‚∏ª

Cargo metadata (publish-ready)

[package]
name = "tdln-brain"
version = "0.1.0"
license = "MIT"
edition = "2021"
description = "Cognitive layer for LogLine OS. Strict TDLN intent generation with reasoning separation."
readme = "README.md"
repository = "https://github.com/LogLine-Foundation/logline-workspace"
homepage  = "https://logline.foundation"
keywords  = ["llm","agent","json","tdln","security"]
categories= ["science::robotics","api-bindings"]

[package.metadata.docs.rs]
all-features = true
rustdoc-args = ["--cfg","docsrs"]


‚∏ª

Roadmap (v0.1 ‚Üí v0.2)
	‚Ä¢	v0.1 (now): cognitive context + parser + mock driver + docs, tests.
	‚Ä¢	v0.1.1: usage telemetry hooks (tracing spans, counters).
	‚Ä¢	v0.2: multi-turn plan mode (structured scratchpad) and red-team ‚Äúself-check‚Äù prompt add-on (pre-Gate sanity).

‚∏ª

One-screen checklist (Proof of Done)
	‚Ä¢	cargo clippy --all-features -- -D warnings ‚úÖ
	‚Ä¢	cargo test (the 5 tests above) ‚úÖ
	‚Ä¢	cargo doc -p tdln-brain builds clean ‚úÖ
	‚Ä¢	README quickstart compiles (doctest or example) ‚úÖ
	‚Ä¢	Publish: cargo publish -p tdln-brain ‚úÖ

‚∏ª

that‚Äôs the brain. want me to drop the complete README.md (with badges and copy-pasteable quickstart) as the next message, or jump straight to crate 2 (ubl-mcp) in the same level of detail?